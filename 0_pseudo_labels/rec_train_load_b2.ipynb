{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 08:58:53.009385: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-15 08:58:53.009408: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-15 08:58:53.010278: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-15 08:58:53.014389: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-15 08:58:53.657050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from memory_profiler import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/media/jczars/4C22F02A22F01B22/Pollen_classification_view/0_pseudo_labels', '/home/jczars/anaconda3/envs/tf/lib/python310.zip', '/home/jczars/anaconda3/envs/tf/lib/python3.10', '/home/jczars/anaconda3/envs/tf/lib/python3.10/lib-dynload', '', '/home/jczars/.local/lib/python3.10/site-packages', '/home/jczars/anaconda3/envs/tf/lib/python3.10/site-packages', '/media/jczars/4C22F02A22F01B22/Pollen_classification_view/']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jczars/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/media/jczars/4C22F02A22F01B22/Pollen_classification_view/0_pseudo_labels', '/home/jczars/anaconda3/envs/tf/lib/python310.zip', '/home/jczars/anaconda3/envs/tf/lib/python3.10', '/home/jczars/anaconda3/envs/tf/lib/python3.10/lib-dynload', '', '/home/jczars/.local/lib/python3.10/site-packages', '/home/jczars/anaconda3/envs/tf/lib/python3.10/site-packages', '/media/jczars/4C22F02A22F01B22/Pollen_classification_view/', '/media/jczars/4C22F02A22F01B22/$WinREAgent/Pollen_classification_view/']\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/media/jczars/4C22F02A22F01B22/Pollen_classification_view/')\n",
    "print(sys.path)\n",
    "\n",
    "from models import get_data, models_train, get_calssifica, utils, reports_build\n",
    "\n",
    "working_dir=\"/media/jczars/4C22F02A22F01B22/Pollen_classification_view/\"\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "def rec_id(workbook_path, id_test):\n",
    "    # Load configuration data from 'Sheet'\n",
    "    config_data = pd.read_excel(workbook_path, sheet_name=\"Sheet\")\n",
    "    config = config_data.loc[id_test]\n",
    "    rec_csv = pd.read_excel(workbook_path, sheet_name=\"Table\")\n",
    "    fil=rec_csv[rec_csv['id_test'] == id_test]\n",
    "    tempo_px=len(fil)\n",
    "\n",
    "    return config, tempo_px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "def train_model(config, train_data, val_data, time_step):\n",
    "    \"\"\"\n",
    "    Train a model with the given configuration and data. If time_step > 0, it will load a pre-trained model\n",
    "    and continue training. Otherwise, it will train a new model from scratch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        A dictionary containing the configuration for model training.\n",
    "    train_data : tuple\n",
    "        A tuple containing the training data and labels.\n",
    "    val_data : tuple\n",
    "        A tuple containing the validation data and labels.\n",
    "    time_step : int\n",
    "        The current training step. If time_step > 0, it will load a model from the previous step.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model_inst : keras.Model\n",
    "        The trained or reloaded model instance.\n",
    "    res_train : dict\n",
    "        A dictionary containing the training history and metrics.\n",
    "    \"\"\"\n",
    "    print('\\n[INFO]--> time_step ', time_step)\n",
    "    \n",
    "    # Reset model_inst to ensure it starts fresh for each time step\n",
    "    model_inst = None\n",
    "    \n",
    "    # If time_step > 0, try to load the model from a previous step, otherwise train a new model\n",
    "    if time_step > 0:\n",
    "        # Build the model path for the previous step\n",
    "        model_name = f\"{config['id_test']}_{config['model']}_bestLoss_{time_step - 1}.keras\"\n",
    "        save_path = os.path.join(config['save_dir'], model_name)\n",
    "        \n",
    "        # Load the model from the previous time step\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"[INFO]--> Loading model from {save_path}\")\n",
    "            model_inst = tf.keras.models.load_model(save_path)\n",
    "            \n",
    "            # Explicitly freeze the layers again\n",
    "            for layer in model_inst.layers:\n",
    "                layer.trainable = False  # Freeze all layers\n",
    "            \n",
    "            # Optionally unfreeze the last few layers if needed (based on config['freeze'])\n",
    "            for i, layer in enumerate(model_inst.layers):\n",
    "                if i >= config['freeze']:\n",
    "                    layer.trainable = True  # Unfreeze layers after the specified freeze index\n",
    "            \n",
    "            print(f\"[INFO]--> Model layers frozen up to layer {config['freeze']}\")\n",
    "        else:\n",
    "            raise ValueError(f\"[ERROR]--> Model from time_step {time_step - 1} not found at {save_path}\")\n",
    "    \n",
    "    # else:\n",
    "    #     # Instantiate the model from scratch for time_step == 0\n",
    "    #     print(\"[INFO]--> Training a new model from scratch...\")\n",
    "    #     model_inst = models_pre.hyper_model_up(config, verbose=1)\n",
    "    \n",
    "    # Train the model with the training and validation data\n",
    "    res_train = models_train.run_train(train_data, val_data, model_inst, config)\n",
    "    \n",
    "    # Save the model at the current time step\n",
    "    model_name = f\"{config['id_test']}_{config['model']}_bestLoss_{time_step}.keras\"\n",
    "    save_path = os.path.join(config['save_dir'], model_name)\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_inst.save(save_path)\n",
    "    print(f\"[INFO]--> Model saved at {save_path}\")\n",
    "    \n",
    "    return model_inst, res_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build_reports_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def build_reports_config(time_step, config, res_pre, model_inst, res_train, verbose=0):\n",
    "    \"\"\"\n",
    "    Generates evaluation reports for a model based on given configurations, test data, and training results.\n",
    "\n",
    "    Parameters:\n",
    "    - time_step (int/float): The time step or timestamp associated with the evaluation.\n",
    "    - config (dict): A dictionary containing model and report configuration parameters, such as image size and batch size.\n",
    "    - res_pre (dict): Contains preprocessing results, including the test data path and category information.\n",
    "    - model_inst: The trained model instance.\n",
    "    - res_train (dict): Contains training results, including training history.\n",
    "    - verbose (int, optional): Level of verbosity for printing messages. Default is 0 (no output).\n",
    "\n",
    "    Returns:\n",
    "    - report_metrics (Any): Generated metrics from the report generation process.\n",
    "\n",
    "    Function Workflow:\n",
    "    1. Conditionally prints log messages indicating the start of report generation based on verbosity level.\n",
    "    2. Loads test data from a specified path.\n",
    "    3. Prepares the input size for data processing.\n",
    "    4. Loads test data for evaluation.\n",
    "    5. Creates necessary directories for report storage.\n",
    "    6. Configures and generates reports using provided data and model.\n",
    "    \"\"\"\n",
    "    if verbose > 0:\n",
    "        print('\\nReports Generation')\n",
    "        print(f'\\n[INFO]--> Step 1.4 - Evaluation Time Step: {time_step}')\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = pd.read_csv(res_pre['path_test'])\n",
    "    print(\"\\n[INFO]--> res_pre['path_test']\", res_pre['path_test'])\n",
    "    print('\\n[INFO]--> test_data.head()', test_data.head())\n",
    "\n",
    "    img_size = config['img_size']\n",
    "    input_size = (img_size, img_size)\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(f'\\n[INFO]--> Input size: {input_size}')\n",
    "    \n",
    "    # Load processed test data\n",
    "    test = get_data.load_data_test(test_data, input_size)\n",
    "    categories = res_pre['categories']\n",
    "    \n",
    "    # Create report saving directory\n",
    "    save_dir = os.path.join(res_pre['save_dir_train'], 'reports')\n",
    "    utils.create_folders(save_dir, 0)\n",
    "    \n",
    "    # Configure report generation settings\n",
    "    reports_config = {\n",
    "        'save_dir': save_dir,\n",
    "        'time': time_step,\n",
    "        'batch_size': config['batch_size'],\n",
    "        'id_test': config['id_test'],\n",
    "        'model': config['model']\n",
    "    }\n",
    "    \n",
    "    # Generate reports\n",
    "    history = res_train['history']\n",
    "    report_metrics = reports_build.reports_gen(test, model_inst, categories, history, reports_config)\n",
    "    \n",
    "    return report_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def classification(config, res_pre, model, _tempo, verbose=0):\n",
    "    \"\"\"\n",
    "    Classifies unlabeled images and generates pseudo-labels.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Model configuration and directory paths.\n",
    "        res_pre (dict): Previous results, including categories and pseudo-label paths.\n",
    "        model (torch.nn.Module): Trained model used for predictions.\n",
    "        _tempo (int): Current iteration of the pseudo-labeling process.\n",
    "        verbose (int, optional): Verbosity level for printing messages. Default is 0 (no output).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing pseudo-label predictions, or None if data is unavailable.\n",
    "    \"\"\"\n",
    "    # Define the path for the unlabeled dataset\n",
    "    unlabels_path = os.path.join(config['path_base'], 'images_unlabels')\n",
    "    batch_size = config['batch_size']\n",
    "    categories = res_pre['categories']\n",
    "\n",
    "    params = {\n",
    "        'unlabels': unlabels_path,\n",
    "        'img_size': config['img_size'],\n",
    "        'batch_size': batch_size,\n",
    "        'categories': categories\n",
    "    }\n",
    "\n",
    "    # Load unlabeled data or read CSV with previous pseudo-labels\n",
    "    if _tempo == 0:\n",
    "        if verbose > 0:\n",
    "            print(f\"[DEBUG] Loading unlabeled images from directory: {unlabels_path}\")\n",
    "        unlabels_generator = get_data.load_unlabels(params)\n",
    "    else:\n",
    "        unlabels_csv_path = os.path.join(res_pre['pseudo_csv'], f'unlabelSet_T{_tempo}.csv')\n",
    "        #/media/jczars/4C22F02A22F01B22/Pollen_classification_view/0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/pseudo_csv/unlabelSet_T14.csv\n",
    "        if verbose > 0:\n",
    "            print(f\"[DEBUG] Loading pseudo-labels from CSV: {unlabels_csv_path}\")\n",
    "        try:\n",
    "            df_unlabels = pd.read_csv(unlabels_csv_path)\n",
    "        except FileNotFoundError:\n",
    "            if verbose > 0:\n",
    "                print(f\"[ERROR] File not found: {unlabels_csv_path}\")\n",
    "            return None\n",
    "\n",
    "        if len(df_unlabels) > 0:\n",
    "            if verbose > 0:\n",
    "                print(\"[DEBUG] Head of the pseudo-labels DataFrame:\")\n",
    "                print(df_unlabels.head())\n",
    "            unlabels_generator = get_data.load_data_test(df_unlabels, input_size=(224, 224))\n",
    "        else:\n",
    "            if verbose > 0:\n",
    "                print(f\"[WARNING] No data found in CSV {unlabels_csv_path}\")\n",
    "            return None\n",
    "\n",
    "    # Perform predictions for pseudo-labeling\n",
    "    if verbose > 0:\n",
    "        print(\"[INFO] Performing pseudo-labeling on the unlabeled dataset\")\n",
    "    \n",
    "    pseudos_df = reports_build.predict_unlabeled_data(\n",
    "        unlabels_generator, model, batch_size, categories, verbose=verbose\n",
    "    )\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(f\"[INFO] Total pseudo-labels generated: {len(pseudos_df)}\")\n",
    "\n",
    "    return pseudos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def selection(pseudos_df, conf, res_pre, _tempo, verbose=0):\n",
    "    \"\"\"\n",
    "    Performs selection of pseudo-labels for training if unlabeled data is available.\n",
    "\n",
    "    Steps:\n",
    "    1. Checks if there is any unlabeled data.\n",
    "    2. Calls the `selec` function to select pseudo-labels based on a confidence threshold.\n",
    "    3. Returns a dictionary with paths and sizes of datasets if selection is successful.\n",
    "    4. Returns None if no selection could be made or if there is no unlabeled data.\n",
    "\n",
    "    Parameters:\n",
    "        pseudos_df (DataFrame): Unlabeled data to be processed.\n",
    "        conf (dict): Configuration dictionary with paths and threshold settings.\n",
    "        res_pre (dict): Contains the path for saving pseudo-labels.\n",
    "        _tempo (int): Current time or iteration index.\n",
    "        training_data (Any): Training data used for comparison or updating with pseudo-labels.\n",
    "        verbose (int, optional): Verbosity level for printing messages. Default is 0 (no output).\n",
    "\n",
    "    Returns:\n",
    "        dict or None: Returns a dictionary with new data paths and dataset sizes if successful; \n",
    "                      returns None if no selection was made or no unlabeled data.\n",
    "    \"\"\"\n",
    "    if not pseudos_df.empty:\n",
    "        if verbose > 0:\n",
    "            print('\\n[STEP 2].4 - Selection')\n",
    "\n",
    "        # Load the training data \n",
    "        try:\n",
    "            training_data = pd.read_csv(res_pre['path_train'])\n",
    "        except (FileNotFoundError, pd.errors.EmptyDataError) as e:\n",
    "            raise ValueError(f\"Error loading training data: {e}\")\n",
    "\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f'Training data provided: {training_data}')\n",
    "\n",
    "        # Perform pseudo-label selection\n",
    "        res_sel = get_calssifica.selec(\n",
    "            conf,\n",
    "            pseudos_df,\n",
    "            res_pre['pseudo_csv'], \n",
    "            _tempo, \n",
    "            training_data,\n",
    "            conf['limiar']\n",
    "        )\n",
    "\n",
    "        if res_sel:\n",
    "            # Return paths and dataset sizes for further processing\n",
    "            return {\n",
    "                '_csv_New_TrainSet': res_sel['_csv_New_TrainSet'],\n",
    "                'path_test': res_pre['path_test'],\n",
    "                'save_dir_train': conf.get('path_model', ''),  # Assuming model save path in config\n",
    "                'pseudo_csv': res_pre['pseudo_csv'],\n",
    "                'ini': res_sel['ini'],\n",
    "                'select': res_sel['select'],\n",
    "                'rest': res_sel['rest'],\n",
    "                'train': res_sel['train'],\n",
    "                'new_train': res_sel['new_train']\n",
    "            }\n",
    "        else:\n",
    "            # No valid pseudo-labels selected\n",
    "            if verbose > 0:\n",
    "                print(\"[INFO] No valid pseudo-labels were selected.\")\n",
    "            return None\n",
    "    else:\n",
    "        # No unlabeled data to process\n",
    "        if verbose > 0:\n",
    "            print(\"[INFO] No unlabeled data available for processing.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def rel_data(time_step, report_metrics, res_train, res_sel, workbook_path, config_index, verbose=0):\n",
    "    \"\"\"\n",
    "    Saves data into an Excel workbook for reporting purposes.\n",
    "\n",
    "    Parameters:\n",
    "        time_step (str/int): Current time step or identifier for data logging.\n",
    "        report_metrics (dict): Metrics from the report generation process.\n",
    "        res_train (dict): Training result data, including timing and accuracy metrics.\n",
    "        res_sel (dict): Selection result data, containing training set sizes and other statistics.\n",
    "        workbook_path (str): Path to the Excel workbook.\n",
    "        config_index (str/int): Identifier for the configuration used.\n",
    "        verbose (int, optional): Verbosity level for printing messages. Default is 0 (no output).\n",
    "    \"\"\"\n",
    "    if verbose > 0:\n",
    "        print(\"\\n[INFO] Workbook name:\", workbook_path)\n",
    "    \n",
    "    try:\n",
    "        workbook = openpyxl.load_workbook(workbook_path)\n",
    "        if verbose > 0:\n",
    "            print(\"Sheets in workbook:\", workbook.sheetnames)\n",
    "    except FileNotFoundError:\n",
    "        if verbose > 0:\n",
    "            print(\"[ERROR] Workbook not found, creating a new one.\")\n",
    "        workbook = openpyxl.Workbook()\n",
    "\n",
    "    sheet_name = 'Table'\n",
    "    \n",
    "    # Check if the sheet already exists\n",
    "    if sheet_name in workbook.sheetnames:\n",
    "        if verbose > 0:\n",
    "            print(f'Sheet \"{sheet_name}\" exists.')\n",
    "        Met_page = workbook[sheet_name]  # Access the existing sheet\n",
    "    else:\n",
    "        if verbose > 0:\n",
    "            print(f'Creating new sheet: \"{sheet_name}\".')\n",
    "        Met_page = workbook.create_sheet(sheet_name)  # Create a new sheet\n",
    "        if verbose > 0:\n",
    "            print('[INFO] -rel_data- Saving test header.')\n",
    "        cols_exe = ['Tempo', 'test_loss', 'test_accuracy', 'precision', 'recall', 'fscore', \n",
    "                    'kappa', 'str_time', 'end_time', 'delay', 'best_epoch', \n",
    "                    'ini', 'select', 'rest', 'train', 'new_train', 'id_test']\n",
    "        Met_page.append(cols_exe)  # Add header row with column names\n",
    "    \n",
    "    # Append data to the sheet\n",
    "    data = [\n",
    "        str(time_step),\n",
    "        report_metrics.get('test_loss', ''),\n",
    "        report_metrics.get('test_accuracy', ''),\n",
    "        report_metrics.get('precision', ''),\n",
    "        report_metrics.get('recall', ''),\n",
    "        report_metrics.get('fscore', ''),\n",
    "        report_metrics.get('kappa', ''),\n",
    "        res_train.get('start_time', ''),\n",
    "        res_train.get('end_time', ''),\n",
    "        res_train.get('duration', ''),\n",
    "        res_train.get('best_epoch', ''),\n",
    "        res_sel.get('ini', ''),\n",
    "        res_sel.get('select', ''),\n",
    "        res_sel.get('rest', ''),\n",
    "        res_sel.get('train', ''),\n",
    "        res_sel.get('new_train', ''),\n",
    "        config_index\n",
    "    ]\n",
    "    Met_page.append(data)\n",
    "    \n",
    "    # Save the workbook\n",
    "    workbook.save(workbook_path)\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(\"Data saved successfully. Sheets available:\", workbook.sheetnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1(workbook_path, id_test, verbose=1):\n",
    "    has_unlabeled_data = True\n",
    "    print('\\n[STEP] Recovery phase')\n",
    "    #%memit  \n",
    "    config, time_step = rec_id(workbook_path, id_test)\n",
    "    print(\"\\nconfig\", config)\n",
    "    print(\"\\ntempo_px\", time_step) \n",
    "\n",
    "    while has_unlabeled_data: \n",
    "        print(f'*'*30)               \n",
    "        print('\\n[STEP] Train phase')\n",
    "\n",
    "        confi_load={\n",
    "            'aug': config['aug'],\n",
    "            'img_size': config['img_size'],\n",
    "        }\n",
    "\n",
    "        # recurarar csv_NewtainSet14\n",
    "        _pseudo_csv=f'{working_dir}/0_pseudo_labels/Reports/{id_test}_{config[\"model\"]}_{config[\"aug\"]}_{config[\"base\"]}/pseudo_csv/'\n",
    "        _csv_New_TrainSet = os.path.join(_pseudo_csv, f'trainSet_T{time_step}.csv')\n",
    "        print(_csv_New_TrainSet)\n",
    "\n",
    "        save_dir = f'{working_dir}/0_pseudo_labels/Reports/{id_test}_{config[\"model\"]}_{config[\"aug\"]}_{config[\"base\"]}/models/'\n",
    "        \n",
    "        config_train={\n",
    "        'id_test': id_test,\n",
    "        'model': config['model'],\n",
    "        'save_dir': save_dir,\n",
    "        'freeze': config['freeze'],\n",
    "        'batch_size': config['batch_size'],\n",
    "        'epochs': config['epochs'],\n",
    "        }\n",
    "\n",
    "        train, val = get_data.reload_data_train(confi_load, _csv_New_TrainSet)\n",
    "\n",
    "        model_train, res_train = train_model(config_train, train, val, time_step)\n",
    "\n",
    "        # Label directory path\n",
    "        labels_dir = os.path.join(config['path_base'], \"labels\")\n",
    "        categories = sorted(os.listdir(labels_dir))\n",
    "\n",
    "        class_config={'path_base': config['path_base'],\n",
    "                    'batch_size': config['batch_size'],\n",
    "                    'img_size': config['img_size'],\n",
    "                    'categories': categories,\n",
    "                    'pseudo_csv': _pseudo_csv\n",
    "        }\n",
    "        #/media/jczars/4C22F02A22F01B22/Pollen_classification_view/0_pseudo_labels/Reports/BI_5_testSet.csv\n",
    "        path_test= f\"{working_dir}/0_pseudo_labels/Reports/{config['base']}_testSet.csv\"\n",
    "        train_path= f\"{working_dir}/0_pseudo_labels/Reports/{config['base']}_trainSet.csv\"\n",
    "        save_dir_train= f'{working_dir}/0_pseudo_labels/Reports/{id_test}_{config[\"model\"]}_{config[\"aug\"]}_{config[\"base\"]}'\n",
    "        num_labels = len(categories)\n",
    "        print(path_test)\n",
    "        res_pre = {\n",
    "            'path_train': train_path,\n",
    "            'path_test': path_test,\n",
    "            'save_dir_train': save_dir_train,\n",
    "            'pseudo_csv': _pseudo_csv,\n",
    "            'size_of_labels': num_labels,\n",
    "            'categories': categories,\n",
    "        }\n",
    "\n",
    "        report_metrics = build_reports_config(time_step, config, res_pre, model_train, res_train)\n",
    "        \n",
    "        print('\\n[STEP] Classification phase')\n",
    "        pseudos_df = classification(class_config, res_pre, model_train, time_step)  \n",
    "\n",
    "        print('\\n[STEP] Selection phase')\n",
    "        res_sel = selection(pseudos_df, config, res_pre, time_step)\n",
    "\n",
    "        if res_sel is None:\n",
    "            if verbose > 0:\n",
    "                print(\"[INFO] No more unlabeled data to process.\")\n",
    "            break\n",
    "        print('\\n[STEP] save reports')\n",
    "        rel_data(time_step, report_metrics, res_train, res_sel, workbook_path, time_step)\n",
    "        \n",
    "        time_step += 1\n",
    "        print(f'*'*30)   \n",
    "        print('\\n[STEP] Next step', time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP] Recovery phase\n",
      "\n",
      "config id_test                      5\n",
      "base                      BI_5\n",
      "path_base              BD/BI_5\n",
      "aug                        sem\n",
      "img_size                   224\n",
      "batch_size                   4\n",
      "epochs                       1\n",
      "alpha                  0.00001\n",
      "model              DenseNet201\n",
      "last_activation        softmax\n",
      "activation                relu\n",
      "dense_size                1024\n",
      "dropout                    0.3\n",
      "freeze                       0\n",
      "split_valid                0.2\n",
      "optimizer              RMSprop\n",
      "learning_rate           0.0001\n",
      "type_model            imagenet\n",
      "limiar                   0.995\n",
      "Name: 5, dtype: object\n",
      "\n",
      "tempo_px 14\n",
      "\n",
      "[STEP] Train phase\n",
      "/media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/pseudo_csv/trainSet_T14.csv\n",
      "training_data_path  /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/pseudo_csv/trainSet_T14.csv\n",
      "\n",
      " ######## Data Generator ################\n",
      "Found 2891 validated image filenames belonging to 6 classes.\n",
      "Found 722 validated image filenames belonging to 6 classes.\n",
      "\n",
      "[INFO]--> time_step  14\n",
      "[INFO]--> Loading model from /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/models/5_DenseNet201_bestLoss_13.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 08:58:56.487352: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-15 08:58:56.513004: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-15 08:58:56.513135: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-15 08:58:56.514001: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-15 08:58:56.514097: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-15 08:58:56.514189: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-15 08:58:56.567851: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-15 08:58:56.567997: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-15 08:58:56.568093: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-15 08:58:56.568163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9993 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]--> Model layers frozen up to layer 0\n",
      "Batch size: 4\n",
      "Training start time: 2024-11-15 09:00:52\n",
      "\n",
      " 2024-11-15 09:00:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 09:01:18.135281: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-11-15 09:01:29.039939: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f233c5d2630 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-15 09:01:29.039954: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-11-15 09:01:29.043397: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731672089.088660   60797 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 135s 675ms/step - loss: 0.0045 - accuracy: 0.9983 - val_loss: 0.4322 - val_accuracy: 0.9349\n",
      "Training duration: 0:02:15\n",
      "Best epoch: 1 with validation loss: 0.4322\n",
      "[INFO]--> Model saved at /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/models/5_DenseNet201_bestLoss_14.keras\n",
      "/media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/BI_5_testSet.csv\n",
      "ERROR: Could not find file /tmp/ipykernel_60705/1460078313.py\n",
      "\n",
      "[INFO]--> res_pre['path_test'] /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/BI_5_testSet.csv\n",
      "\n",
      "[INFO]--> test_data.head()                                                 file            labels\n",
      "0  BD/BI_5/labels/polar_circular/9.Origanum (46).png    polar_circular\n",
      "1  BD/BI_5/labels/polar_triangular/12.Calicotome ...  polar_triangular\n",
      "2  BD/BI_5/labels/polar_circular/9.Origanum (11).png    polar_circular\n",
      "3  BD/BI_5/labels/polar_circular/10.Satureja (46)...    polar_circular\n",
      "4   BD/BI_5/labels/polar_circular/1.Thymbra (19).png    polar_circular\n",
      "Found 127 validated image filenames belonging to 6 classes.\n",
      "folders test already exists:  /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/models/reports\n",
      "4/4 [==============================] - 1s 349ms/step - loss: 0.6398 - accuracy: 0.9291\n",
      " 4/32 [==>...........................] - ETA: 2sWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 32 batches). You may need to use the repeat() function when building your dataset.\n",
      "32/32 [==============================] - 7s 15ms/step\n",
      "Size y_true: 127\n",
      "Size y_pred: 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/jczars/4C22F02A22F01B22/Pollen_classification_view/models/reports_build.py:315: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(ax.get_yticklabels(), fontsize=10, rotation=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP] Classification phase\n",
      "ERROR: Could not find file /tmp/ipykernel_60705/1881900816.py\n",
      "Found 66 validated image filenames belonging to 5 classes.\n",
      "Predicting unlabeled data... 66\n",
      " 3/17 [====>.........................] - ETA: 3sWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 17 batches). You may need to use the repeat() function when building your dataset.\n",
      "17/17 [==============================] - 1s 35ms/step\n",
      "\n",
      "[STEP] Selection phase\n",
      "ERROR: Could not find file /tmp/ipykernel_60705/2036738533.py\n",
      "                                                file             labels  \\\n",
      "0  BD/BI_5/images_unlabels/unlabels/1.Thymbra (11...   polar_triangular   \n",
      "1  BD/BI_5/images_unlabels/unlabels/1.Thymbra (13...   polar_triangular   \n",
      "2  BD/BI_5/images_unlabels/unlabels/1.Thymbra (14...  polar_tricircular   \n",
      "3  BD/BI_5/images_unlabels/unlabels/1.Thymbra (28...     polar_circular   \n",
      "4  BD/BI_5/images_unlabels/unlabels/1.Thymbra (51...   polar_triangular   \n",
      "\n",
      "   confidence  \n",
      "0    0.355792  \n",
      "1    0.895780  \n",
      "2    0.518683  \n",
      "3    0.999879  \n",
      "4    0.827282  \n",
      "Initial Data Preview:                                                 file             labels  \\\n",
      "0  BD/BI_5/images_unlabels/unlabels/1.Thymbra (11...   polar_triangular   \n",
      "1  BD/BI_5/images_unlabels/unlabels/1.Thymbra (13...   polar_triangular   \n",
      "2  BD/BI_5/images_unlabels/unlabels/1.Thymbra (14...  polar_tricircular   \n",
      "3  BD/BI_5/images_unlabels/unlabels/1.Thymbra (28...     polar_circular   \n",
      "4  BD/BI_5/images_unlabels/unlabels/1.Thymbra (51...   polar_triangular   \n",
      "\n",
      "   confidence  \n",
      "0    0.355792  \n",
      "1    0.895780  \n",
      "2    0.518683  \n",
      "3    0.999879  \n",
      "4    0.827282  \n",
      "Filtered data size: 27\n",
      "Remaining unlabeled data size: 39\n",
      "Saving remaining unlabeled data to /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/pseudo_csv/unlabelSet_T15.csv\n",
      "Saving new training set to /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/pseudo_csv/trainSet_T15.csv\n",
      "ERROR: Could not find file /tmp/ipykernel_60705/107343231.py\n",
      "\n",
      "[STEP] Next step 15\n",
      "\n",
      "[STEP] Train phase\n",
      "/media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/pseudo_csv/trainSet_T15.csv\n",
      "training_data_path  /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/pseudo_csv/trainSet_T15.csv\n",
      "\n",
      " ######## Data Generator ################\n",
      "Found 2912 validated image filenames belonging to 6 classes.\n",
      "Found 728 validated image filenames belonging to 6 classes.\n",
      "\n",
      "[INFO]--> time_step  15\n",
      "[INFO]--> Loading model from /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/models/5_DenseNet201_bestLoss_14.keras\n",
      "[INFO]--> Model layers frozen up to layer 0\n",
      "Batch size: 4\n",
      "Training start time: 2024-11-15 09:05:16\n",
      "\n",
      " 2024-11-15 09:05:16\n",
      "91/91 [==============================] - 75s 394ms/step - loss: 0.0148 - accuracy: 0.9938 - val_loss: 0.2731 - val_accuracy: 0.9643\n",
      "Training duration: 0:01:15\n",
      "Best epoch: 1 with validation loss: 0.2731\n",
      "[INFO]--> Model saved at /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/models/5_DenseNet201_bestLoss_15.keras\n",
      "/media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/BI_5_testSet.csv\n",
      "ERROR: Could not find file /tmp/ipykernel_60705/1460078313.py\n",
      "\n",
      "[INFO]--> res_pre['path_test'] /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/BI_5_testSet.csv\n",
      "\n",
      "[INFO]--> test_data.head()                                                 file            labels\n",
      "0  BD/BI_5/labels/polar_circular/9.Origanum (46).png    polar_circular\n",
      "1  BD/BI_5/labels/polar_triangular/12.Calicotome ...  polar_triangular\n",
      "2  BD/BI_5/labels/polar_circular/9.Origanum (11).png    polar_circular\n",
      "3  BD/BI_5/labels/polar_circular/10.Satureja (46)...    polar_circular\n",
      "4   BD/BI_5/labels/polar_circular/1.Thymbra (19).png    polar_circular\n",
      "Found 127 validated image filenames belonging to 6 classes.\n",
      "folders test already exists:  /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/models/reports\n",
      "4/4 [==============================] - 1s 121ms/step - loss: 0.5609 - accuracy: 0.9291\n",
      " 4/32 [==>...........................] - ETA: 2sWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 32 batches). You may need to use the repeat() function when building your dataset.\n",
      "32/32 [==============================] - 7s 15ms/step\n",
      "Size y_true: 127\n",
      "Size y_pred: 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/jczars/4C22F02A22F01B22/Pollen_classification_view/models/reports_build.py:315: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(ax.get_yticklabels(), fontsize=10, rotation=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP] Classification phase\n",
      "ERROR: Could not find file /tmp/ipykernel_60705/1881900816.py\n",
      "Found 39 validated image filenames belonging to 6 classes.\n",
      "Predicting unlabeled data... 39\n",
      " 2/10 [=====>........................] - ETA: 4sWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
      "10/10 [==============================] - 1s 60ms/step\n",
      "\n",
      "[STEP] Selection phase\n",
      "ERROR: Could not find file /tmp/ipykernel_60705/2036738533.py\n",
      "                                                file               labels  \\\n",
      "0  BD/BI_5/images_unlabels/unlabels/1.Thymbra (11...  equatorial_circular   \n",
      "1  BD/BI_5/images_unlabels/unlabels/1.Thymbra (13...  equatorial_circular   \n",
      "2  BD/BI_5/images_unlabels/unlabels/1.Thymbra (14...       polar_circular   \n",
      "3  BD/BI_5/images_unlabels/unlabels/1.Thymbra (51...       polar_circular   \n",
      "4  BD/BI_5/images_unlabels/unlabels/1.Thymbra (86...       polar_circular   \n",
      "\n",
      "   confidence  \n",
      "0    0.715262  \n",
      "1    0.999151  \n",
      "2    0.928889  \n",
      "3    0.700651  \n",
      "4    0.986535  \n",
      "Initial Data Preview:                                                 file               labels  \\\n",
      "0  BD/BI_5/images_unlabels/unlabels/1.Thymbra (11...  equatorial_circular   \n",
      "1  BD/BI_5/images_unlabels/unlabels/1.Thymbra (13...  equatorial_circular   \n",
      "2  BD/BI_5/images_unlabels/unlabels/1.Thymbra (14...       polar_circular   \n",
      "3  BD/BI_5/images_unlabels/unlabels/1.Thymbra (51...       polar_circular   \n",
      "4  BD/BI_5/images_unlabels/unlabels/1.Thymbra (86...       polar_circular   \n",
      "\n",
      "   confidence  \n",
      "0    0.715262  \n",
      "1    0.999151  \n",
      "2    0.928889  \n",
      "3    0.700651  \n",
      "4    0.986535  \n",
      "Filtered data size: 11\n",
      "Remaining unlabeled data size: 28\n",
      "Saving remaining unlabeled data to /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/pseudo_csv/unlabelSet_T16.csv\n",
      "Saving new training set to /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/pseudo_csv/trainSet_T16.csv\n",
      "ERROR: Could not find file /tmp/ipykernel_60705/107343231.py\n",
      "\n",
      "[STEP] Next step 16\n",
      "\n",
      "[STEP] Train phase\n",
      "/media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/pseudo_csv/trainSet_T16.csv\n",
      "training_data_path  /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/pseudo_csv/trainSet_T16.csv\n",
      "\n",
      " ######## Data Generator ################\n",
      "Found 2921 validated image filenames belonging to 6 classes.\n",
      "Found 730 validated image filenames belonging to 6 classes.\n",
      "\n",
      "[INFO]--> time_step  16\n",
      "[INFO]--> Loading model from /media/jczars/4C22F02A22F01B22/Pollen_classification_view//0_pseudo_labels/Reports/5_DenseNet201_sem_BI_5/models/5_DenseNet201_bestLoss_15.keras\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    workbook_path = \"/media/jczars/4C22F02A22F01B22/Pollen_classification_view/0_pseudo_labels/Reports/config_pseudo_label_pre.xlsx\"\n",
    "    start_index = 5\n",
    "\n",
    "    test1(workbook_path, start_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
